# 🚀 병렬 웹 스크래퍼 (Parallel Web Scraper)

고성능 병렬 처리를 통한 웹사이트 크롤링 및 오프라인 저장 도구입니다.

## ✨ 주요 기능

- **병렬 처리**: 메모리가 허용하는 한 최대한 많은 브라우저 인스턴스를 동시에 실행
- **2단계 처리**: 
  1. 모든 링크와 에셋 수집 (중복 제거)
  2. 수집된 모든 리소스를 병렬로 저장
- **상대 경로 변환**: 저장된 HTML 파일의 링크를 상대 경로로 변환하여 오프라인 네비게이션 가능
- **깊이 제어**: 최대 크롤링 깊이 설정 가능
- **진행률 표시**: 실시간 진행률 및 통계 정보 제공

## 🏗️ 아키텍처

```
main.js
├── src/
│   ├── config.js (설정 및 CLI 파싱)
│   ├── crawling-manager.js (크롤링 큐 관리)
│   ├── link-collector.js (링크 및 에셋 수집)
│   ├── parallel-processor.js (병렬 저장 처리)
│   ├── browser.js (브라우저 관리)
│   └── utils.js (유틸리티 함수)
```

## 📦 설치

```bash
npm install
```

## 🚀 사용법

### 기본 사용법

```bash
npm run start
```

### 옵션과 함께 사용

```bash
npm run start -- -u https://example.com -o ./output -d 3 --headless
```

### CLI 옵션

| 옵션 | 설명 | 기본값 |
|------|------|--------|
| `-u, --url <URL>` | 크롤링할 기본 URL | `https://amuz.co.kr` |
| `-o, --output <DIR>` | 출력 디렉토리 | `dist` |
| `-d, --depth <NUMBER>` | 최대 크롤링 깊이 | `5` |
| `--headless` | 헤드리스 모드로 실행 | `false` |
| `-h, --help` | 도움말 표시 | - |

### 사용 예시

```bash

# 특정 사이트 크롤링 (깊이 3)
npm run start -- --url https://example.com --output ./example-site --depth 3

# 헤드리스 모드로 실행
npm run start -- -u https://example.com --headless
```

## 🔧 동작 방식

### 1단계: 링크 수집
- 시작 URL에서 모든 내부 링크와 에셋 URL을 수집
- 수집된 링크들을 큐에 추가하고 다음 깊이로 진행
- 중복 링크는 자동으로 제거
- 설정된 최대 깊이까지 반복

### 2단계: 병렬 저장
- 수집된 모든 페이지와 에셋을 병렬로 저장
- HTML 파일의 링크를 상대 경로로 변환
- 진행률을 실시간으로 표시

## 📊 출력 구조

```
output/
├── index.html          # 메인 페이지
├── docs/
│   ├── index.html      # 문서 페이지
│   └── guide.html      # 가이드 페이지
├── static/
│   ├── css/
│   ├── js/
│   └── images/
└── crawl-report.json   # 크롤링 리포트
```

## 🎯 주요 개선사항

### 이전 버전 대비 개선점

1. **병렬 처리**: 순차 처리에서 병렬 처리로 변경하여 성능 대폭 향상
2. **모듈화**: 단일 파일에서 여러 모듈로 분리하여 유지보수성 향상
3. **메모리 효율성**: 브라우저 풀을 사용하여 메모리 사용량 최적화
4. **에러 처리**: Promise.allSettled를 사용하여 일부 실패해도 전체 프로세스 계속 진행
5. **진행률 표시**: 실시간 진행률 및 통계 정보 제공

### 성능 최적화

- **브라우저 풀**: 설정된 동시 처리 수만큼 브라우저 인스턴스 미리 생성
- **청크 처리**: 대량의 URL을 청크 단위로 나누어 처리
- **중복 제거**: Set을 사용하여 중복 URL 자동 제거
- **메모리 관리**: 각 단계별로 불필요한 리소스 정리

## 🔍 모듈 설명

### CrawlingManager
- URL 큐 관리
- 방문한 URL 추적
- 에셋 URL 수집
- 크롤링 상태 관리

### LinkCollector
- 페이지에서 링크와 에셋 URL 추출
- 병렬로 여러 페이지 처리
- 수집된 링크 목록 출력

### ParallelProcessor
- 페이지와 에셋을 병렬로 저장
- HTML 링크를 상대 경로로 변환
- 진행률 표시

## 📈 성능 지표

- **동시 처리**: 최대 10개 브라우저 인스턴스 동시 실행
- **메모리 사용량**: 브라우저 풀을 통한 효율적인 메모리 관리
- **처리 속도**: 병렬 처리로 순차 처리 대비 5-10배 향상

## 🛠️ 개발 환경

- Node.js 18+
- Puppeteer
- ES6+ 모듈 시스템

## 📝 라이선스

MIT License

## 📝 패치노트

### v1.2.5 (2024-12-19)
- **Windows 브라우저 안정성 대폭 개선**: 
  - 브라우저 초기화 로직 완전 재작성
  - `setViewport` 제거로 세션 닫힘 문제 해결
  - Windows 전용 User-Agent 설정
  - 더 간단하고 안정적인 브라우저 인자 사용
  - 2단계 폴백 시스템으로 안정성 확보

### v1.2.4 (2024-12-19)
- **Windows 프로토콜 타임아웃 문제 해결**: 
  - `protocolTimeout: 60000` 설정 추가로 Network.enable 타임아웃 해결
  - 경로 검증 로직 강화 (`chrome.exe` 끝 확인)
  - 페이지 생성 시 예외 처리 추가

### v1.2.3 (2024-12-19)
- **변수 스코프 오류 수정**: 
  - `simplifiedArgs` 변수 스코프 문제 해결
  - 브라우저 실행 시 ReferenceError 오류 수정

### v1.2.2 (2024-12-19)
- **Windows 브라우저 실행 경로 문제 추가 수정**: 
  - Chrome 실행 경로의 공백 문제 해결 (`trim()` 및 정규식으로 공백 제거)
  - 3단계 폴백 시스템 구현으로 브라우저 실행 안정성 향상
  - Windows 환경에서 더 간소화된 브라우저 인자 사용

### v1.2.1 (2024-12-19)
- **Windows 호환성 개선**: Windows 환경에서 Chrome 실행 경로 문제 해결
  - `puppeteer.executablePath()` 사용으로 플랫폼 독립적인 Chrome 경로 감지
  - 하드코딩된 경로 제거로 OS 업데이트에 대한 유연성 향상
  - Windows에서 발생하던 타임아웃 오류 해결
- **코드 개선**: 
  - `src/config.js`: Chrome 경로 감지 로직 단순화
  - `src/browser.js`: 플랫폼별 분기 코드 제거로 일관성 향상

### v1.2.0
- 병렬 처리 기능 추가
- 모듈화된 아키텍처로 리팩토링
- 성능 최적화 및 메모리 효율성 개선

## 🤝 기여

버그 리포트나 기능 제안은 이슈를 통해 해주세요. 