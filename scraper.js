const puppeteer = require('puppeteer');
const fs = require('fs-extra');
const path = require('path');
const { URL } = require('url');

// CLI ì¸ì íŒŒì‹± í•¨ìˆ˜
function parseArguments() {
    const args = process.argv.slice(2);
    const options = {
        baseUrl: 'https://amuz.co.kr',
        outputDir: 'dist',
        maxDepth: 5,
        headless: false
    };

    for (let i = 0; i < args.length; i++) {
        const arg = args[i];
        
        switch (arg) {
            case '--url':
            case '-u':
                options.baseUrl = args[++i] || options.baseUrl;
                break;
            case '--output':
            case '-o':
                options.outputDir = args[++i] || options.outputDir;
                break;
            case '--depth':
            case '-d':
                const depth = parseInt(args[++i]);
                if (!isNaN(depth) && depth > 0) {
                    options.maxDepth = depth;
                }
                break;
            case '--headless':
                options.headless = true;
                break;
            case '--help':
            case '-h':
                showHelp();
                process.exit(0);
                break;
        }
    }

    return options;
}

function showHelp() {
    console.log(`
ğŸ” Amuz Scraper - ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë„êµ¬

ì‚¬ìš©ë²•:
  node scraper.js [ì˜µì…˜]

ì˜µì…˜:
  -u, --url <URL>        í¬ë¡¤ë§í•  ê¸°ë³¸ URL (ê¸°ë³¸ê°’: https://amuz.co.kr)
  -o, --output <DIR>     ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: dist)
  -d, --depth <NUMBER>   ìµœëŒ€ í¬ë¡¤ë§ ê¹Šì´ (ê¸°ë³¸ê°’: 5)
  --headless             í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰
  -h, --help             ë„ì›€ë§ í‘œì‹œ

ì˜ˆì‹œ:
  node scraper.js
  node scraper.js -u https://example.com -o ./output -d 3
  node scraper.js --url https://amuz.co.kr --output ./amuz-offline --depth 5
`);
}

class AmuzScraper {
    constructor(options = {}) {
        this.baseUrl = options.baseUrl || 'https://amuz.co.kr';
        this.maxDepth = options.maxDepth || 5;
        this.outputDir = options.outputDir || 'dist';
        this.headless = options.headless || false;
        this.visitedUrls = new Set();
        this.urlQueue = [];
        this.browser = null;
        this.page = null;
    }

    async init() {
        console.log('ğŸš€ ë¸Œë¼ìš°ì €ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...');
        console.log(`ğŸ“ ëŒ€ìƒ URL: ${this.baseUrl}`);
        console.log(`ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: ${this.outputDir}`);
        console.log(`ğŸ” ìµœëŒ€ ê¹Šì´: ${this.maxDepth}`);
        
        this.browser = await puppeteer.launch({
            headless: this.headless,
            args: [
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-accelerated-2d-canvas',
                '--no-first-run',
                '--no-zygote',
                '--disable-gpu'
            ]
        });

        this.page = await this.browser.newPage();
        
        // ì‚¬ìš©ì ì—ì´ì „íŠ¸ ì„¤ì •
        await this.page.setUserAgent('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36');
        
        // ë·°í¬íŠ¸ ì„¤ì •
        await this.page.setViewport({ width: 1920, height: 1080 });

        // ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        await fs.ensureDir(this.outputDir);
        console.log(`ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: ${this.outputDir}`);
    }

    async extractLinks(pageUrl, depth = 0) {
        if (depth > this.maxDepth || this.visitedUrls.has(pageUrl)) {
            return [];
        }

        console.log(`ğŸ” ${depth}ëìŠ¤ - ${pageUrl} í¬ë¡¤ë§ ì¤‘...`);
        this.visitedUrls.add(pageUrl);

        try {
            // í˜ì´ì§€ ë¡œë“œ
            await this.page.goto(pageUrl, { 
                waitUntil: 'networkidle2',
                timeout: 30000 
            });

            // CSR ë Œë”ë§ì„ ìœ„í•œ ëŒ€ê¸°
            await this.page.waitForTimeout(3000);

            // ëª¨ë“  ë§í¬ ì¶”ì¶œ
            const links = await this.page.evaluate((baseUrl) => {
                const anchors = document.querySelectorAll('a[href]');
                const extractedLinks = [];

                anchors.forEach(anchor => {
                    const href = anchor.href;
                    if (href && href.startsWith(baseUrl)) {
                        extractedLinks.push({
                            url: href,
                            text: anchor.textContent.trim(),
                            title: anchor.title || ''
                        });
                    }
                });

                return extractedLinks;
            }, this.baseUrl);

            console.log(`âœ… ${pageUrl}ì—ì„œ ${links.length}ê°œì˜ ë§í¬ ë°œê²¬`);

            // í˜„ì¬ í˜ì´ì§€ ì €ì¥
            await this.savePage(pageUrl, depth);

            // ë‹¤ìŒ ëìŠ¤ë¡œ íì— ì¶”ê°€
            if (depth < this.maxDepth) {
                for (const link of links) {
                    if (!this.visitedUrls.has(link.url)) {
                        this.urlQueue.push({ url: link.url, depth: depth + 1 });
                    }
                }
            }

            return links;

        } catch (error) {
            console.error(`âŒ ${pageUrl} í¬ë¡¤ë§ ì‹¤íŒ¨:`, error.message);
            return [];
        }
    }

    async savePage(pageUrl, depth) {
        try {
            // í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°
            const html = await this.page.content();
            
            // URLì„ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜
            const urlObj = new URL(pageUrl);
            let fileName = urlObj.pathname;
            if (fileName === '/') fileName = '/index';
            if (!fileName.endsWith('.html')) fileName += '.html';
            
            // íŒŒì¼ ê²½ë¡œ ìƒì„±
            const filePath = path.join(this.outputDir, `${depth}_depth`, fileName);
            await fs.ensureDir(path.dirname(filePath));

            // HTML ì €ì¥
            await fs.writeFile(filePath, html, 'utf8');
            console.log(`ğŸ’¾ ì €ì¥ë¨: ${filePath}`);

            // ë©”íƒ€ë°ì´í„° ì €ì¥
            const metadata = {
                url: pageUrl,
                depth: depth,
                savedAt: new Date().toISOString(),
                filePath: filePath
            };

            const metadataPath = filePath.replace('.html', '.json');
            await fs.writeFile(metadataPath, JSON.stringify(metadata, null, 2), 'utf8');

        } catch (error) {
            console.error(`âŒ í˜ì´ì§€ ì €ì¥ ì‹¤íŒ¨ (${pageUrl}):`, error.message);
        }
    }

    async crawl() {
        console.log(`ğŸ¯ ${this.baseUrl} í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ ${this.maxDepth}ëìŠ¤)`);
        
        // ì‹œì‘ URLì„ íì— ì¶”ê°€
        this.urlQueue.push({ url: this.baseUrl, depth: 0 });

        while (this.urlQueue.length > 0) {
            const { url, depth } = this.urlQueue.shift();
            
            if (!this.visitedUrls.has(url)) {
                await this.extractLinks(url, depth);
                
                // ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                await new Promise(resolve => setTimeout(resolve, 1000));
            }
        }

        console.log(`âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ ${this.visitedUrls.size}ê°œ í˜ì´ì§€ ë°©ë¬¸`);
    }

    async generateReport() {
        const report = {
            totalPages: this.visitedUrls.size,
            maxDepth: this.maxDepth,
            baseUrl: this.baseUrl,
            outputDir: this.outputDir,
            crawledAt: new Date().toISOString(),
            visitedUrls: Array.from(this.visitedUrls)
        };

        const reportPath = path.join(this.outputDir, 'crawl-report.json');
        await fs.writeFile(reportPath, JSON.stringify(report, null, 2), 'utf8');
        console.log(`ğŸ“Š í¬ë¡¤ë§ ë¦¬í¬íŠ¸ ì €ì¥: ${reportPath}`);
    }

    async close() {
        if (this.browser) {
            await this.browser.close();
            console.log('ğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ');
        }
    }
}

async function main() {
    const options = parseArguments();
    const scraper = new AmuzScraper(options);
    
    try {
        await scraper.init();
        await scraper.crawl();
        await scraper.generateReport();
    } catch (error) {
        console.error('âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:', error);
        process.exit(1);
    } finally {
        await scraper.close();
    }
}

// ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
if (require.main === module) {
    main().catch(console.error);
}

module.exports = AmuzScraper; 